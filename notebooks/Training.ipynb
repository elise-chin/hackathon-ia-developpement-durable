{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bef6b1a-18a3-4c43-9b63-927b35abcb71",
   "metadata": {},
   "source": [
    "## Prérequis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7c9c12-58ad-4de3-9807-e12ad8941b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kerighan/textgraph\n",
      "  Cloning https://github.com/kerighan/textgraph to /tmp/pip-req-build-ans8ld2m\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kerighan/textgraph /tmp/pip-req-build-ans8ld2m\n",
      "  Resolved https://github.com/kerighan/textgraph to commit 6312c29eb43ab83858f6e7ae66f602101fe3f04b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: textgraph\n",
      "  Building wheel for textgraph (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for textgraph: filename=textgraph-0.0.0-py3-none-any.whl size=4984 sha256=15e18af2951ac22349e5e953cb5f390f4edbf6ecc00c7706e168857b81ffe31d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bqmqwvp4/wheels/0c/77/ea/79e80176903744e8f6bdeda8083a0e0e4fc1331fa5647542fa\n",
      "Successfully built textgraph\n",
      "Installing collected packages: textgraph\n",
      "Successfully installed textgraph-0.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openpyxl\n",
      "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (0.24.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.19.4)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.0\n",
      "    Uninstalling scikit-learn-0.24.0:\n",
      "      Successfully uninstalled scikit-learn-0.24.0\n",
      "Successfully installed scikit-learn-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bert_embedding\n",
      "  Downloading bert_embedding-1.0.1-py3-none-any.whl (13 kB)\n",
      "Collecting mxnet==1.4.0\n",
      "  Downloading mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.6/29.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.14.6\n",
      "  Downloading numpy-1.14.6.zip (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing==3.6.6\n",
      "  Downloading typing-3.6.6-py3-none-any.whl (25 kB)\n",
      "Collecting gluonnlp==0.6.0\n",
      "  Downloading gluonnlp-0.6.0.tar.gz (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet==1.4.0->bert_embedding) (2.27.1)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (1.26.8)\n",
      "Building wheels for collected packages: gluonnlp, numpy\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-py3-none-any.whl size=259930 sha256=dc79680a8c901cd29e06a904ad0373feb797f0790072268fef723c287b7cd0bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/d0/a1/57ea55532e4ff6e3efbec7a851724a8f7a5b073ff648dd4160\n",
      "  Building wheel for numpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for numpy: filename=numpy-1.14.6-cp38-cp38-linux_x86_64.whl size=9740558 sha256=860b24d008966e60862da29a33f8ec1fc0ab8b4a9c72dc6929d5e1aef56cc28b\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/81/aa/e309a6725c1cb6f5b37c3c67b74828fd4db0827592ff4a4f85\n",
      "Successfully built gluonnlp numpy\n",
      "Installing collected packages: typing, numpy, graphviz, mxnet, gluonnlp, bert_embedding\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.4\n",
      "    Uninstalling numpy-1.19.4:\n",
      "      Successfully uninstalled numpy-1.19.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.9.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
      "scikit-learn 1.1.0 requires numpy>=1.17.3, but you have numpy 1.14.6 which is incompatible.\n",
      "pyarrow 5.0.0 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
      "pandas 1.2.5 requires numpy>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n",
      "numba 0.53.1 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
      "matplotlib 3.5.0 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
      "h5py 3.1.0 requires numpy>=1.17.5; python_version == \"3.8\", but you have numpy 1.14.6 which is incompatible.\n",
      "dask-cuda 21.12.0 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
      "cupy-cuda115 9.6.0 requires numpy<1.24,>=1.17, but you have numpy 1.14.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bert_embedding-1.0.1 gluonnlp-0.6.0 graphviz-0.8.4 mxnet-1.4.0 numpy-1.14.6 typing-3.6.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch\n",
      "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/kerighan/convectors\n",
      "  Cloning https://github.com/kerighan/convectors to /tmp/pip-req-build-1qypslrg\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kerighan/convectors /tmp/pip-req-build-1qypslrg\n",
      "  Resolved https://github.com/kerighan/convectors to commit 117697b275363daf45c85d75fe97201e0099ef7a\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from convectors==0.1.0) (1.14.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from convectors==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from convectors==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from convectors==0.1.0) (4.62.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from convectors==0.1.0) (1.2.5)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from convectors==0.1.0) (0.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->convectors==0.1.0) (2.8.2)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->convectors==0.1.0) (2021.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->convectors==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->convectors==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->convectors==0.1.0) (1.15.0)\n",
      "Building wheels for collected packages: convectors\n",
      "  Building wheel for convectors (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for convectors: filename=convectors-0.1.0-py3-none-any.whl size=2991046 sha256=aa4f83f3380e1ca7022ca6b3518eae4e2cf24c260a9808fa00661e7687f1e6da\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5n0fq234/wheels/e0/2b/3e/b1ae06f6eb771599f016c47b6d1709d7be3e6f4eb7ae8adee5\n",
      "Successfully built convectors\n",
      "Installing collected packages: numpy, convectors\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.14.6\n",
      "    Uninstalling numpy-1.14.6:\n",
      "      Successfully uninstalled numpy-1.14.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mxnet 1.4.0 requires numpy<1.15.0,>=1.8.2, but you have numpy 1.22.3 which is incompatible.\n",
      "bert-embedding 1.0.1 requires numpy==1.14.6, but you have numpy 1.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed convectors-0.1.0 numpy-1.22.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kerighan/textgraph\n",
    "!pip install openpyxl\n",
    "!pip install scikit-learn -U\n",
    "!pip install bert_embedding\n",
    "!pip install torch\n",
    "!pip install git+https://github.com/kerighan/convectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba32704-7225-4eb7-9d1a-45f65ccb9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textgraph.graph import TextGraph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb3858-506b-48dc-8dfb-989e37fe69bb",
   "metadata": {},
   "source": [
    "## Constitution du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3fa0fcf-c443-4f30-bdac-0e8423652c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('data/afd_snaps_labeled_cibles.xlsx')\n",
    "df12 = pd.read_excel(xls, 'SDG 12')\n",
    "df15 = pd.read_excel(xls, 'SDG 15')\n",
    "df16 = pd.read_excel(xls, 'SDG 16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dec105d-4a8c-473d-9434-3328304ed6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df = pd.read_csv(\"data/raw_filtered.csv\")\n",
    "def get_unlabelled(data, SDG):\n",
    "    \n",
    "    data = data[data[\"SDG\"+str(SDG)] == 1].drop(columns = \"Unnamed: 0\").reset_index(drop = True)\n",
    "    data.columns = [\"Text\"] + list(data.columns[1:])\n",
    "    data = data[[\"Text\"]]\n",
    "    data.Text = [re.sub(\"[^a-zA-Z0-9]\", \" \", text) for text in data.Text]\n",
    "    empty_cols = [\"\" for i in range(len(data))]\n",
    "    data[\"Manual_1\"] =empty_cols\n",
    "    data[\"Manual_2 \"] = empty_cols\n",
    "    return(data)\n",
    "\n",
    "    \n",
    "unlabelled_df12 = get_unlabelled(unlabelled_df, 12)\n",
    "unlabelled_df15 = get_unlabelled(unlabelled_df, 15)\n",
    "unlabelled_df16 = get_unlabelled(unlabelled_df, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08aef36-23b8-4f6e-a2eb-7b6d3977e45f",
   "metadata": {},
   "source": [
    "## Aggrégation des données scrappées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b45a8b-9f35-41bb-b0f8-2d065f85aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_aug = pd.read_csv(\"data/unlabeled_snaps.csv\", sep = \";\")\n",
    "\n",
    "unlabelled_aug_df12 = get_unlabelled(unlabelled_aug, 12)\n",
    "unlabelled_aug_df15 = get_unlabelled(unlabelled_aug, 15)\n",
    "unlabelled_aug_df16 = get_unlabelled(unlabelled_aug, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa978d31-df11-4f1c-b360-be559a8e36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_desc_12 = pd.read_csv(\"data/aug_cibles_12.csv\").drop(columns = \"Unnamed: 0\")\n",
    "labelled_desc_15 = pd.read_csv(\"data/aug_cibles_15.csv\").drop(columns = \"Unnamed: 0\")\n",
    "labelled_desc_16 = pd.read_csv(\"data/aug_cibles_16.csv\").drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf72976-8d13-42cd-93f0-b897dc8144fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df12 = pd.concat([df12,labelled_desc_12, unlabelled_aug_df12, unlabelled_df12], axis = 0).reset_index(drop = True)\n",
    "full_df15 = pd.concat([df15,labelled_desc_15, unlabelled_aug_df15, unlabelled_df15], axis = 0).reset_index(drop = True)\n",
    "full_df16 = pd.concat([df16,labelled_desc_16, unlabelled_aug_df16, unlabelled_df16], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05414438-f720-43a4-8573-91e14537e7f6",
   "metadata": {},
   "source": [
    "## Calcul de l'embedding de chaque mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e64a81-c3fe-43c4-9e3d-1ca62c50d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_wv():\n",
    "    with open(\"wv.pkl\", \"rb\") as f:\n",
    "        wv = pickle.load(f)\n",
    "    return wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6e0985-1a52-4210-b817-b8bb1c102977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_wv(wv):\n",
    "    with open(\"wv.pkl\", \"wb\") as f:\n",
    "        pickle.dump(wv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1094a0-fe4a-491d-8f15-b6adcaafda0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with 12\n",
      "done with 15\n",
      "done with 16\n"
     ]
    }
   ],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "def generate_embeddings(df, first_time = False):\n",
    "    df.Text = [text.replace(\".\",\"\").replace(\",\", \"\").lower() for text in df.Text]\n",
    "    if first_time:\n",
    "        wv = {}\n",
    "    else:\n",
    "        wv = load_wv()\n",
    "    new_words = []\n",
    "    for text in df[\"Text\"]:\n",
    "        sent = text.split()\n",
    "        for word in sent:\n",
    "            if word in wv or word in new_words:\n",
    "                continue\n",
    "            new_words.append(word)\n",
    "    new_words = list(set(new_words))\n",
    "    bert_embedding = BertEmbedding()\n",
    "    result = bert_embedding(new_words)\n",
    "    for pair in result:\n",
    "\n",
    "        wv[pair[0][0]] = pair[1][0]\n",
    "    save_wv(wv)\n",
    "    return wv\n",
    "\n",
    "wv = generate_embeddings(full_df12)\n",
    "print(\"done with 12\")\n",
    "wv = generate_embeddings(full_df15)\n",
    "print(\"done with 15\")\n",
    "wv = generate_embeddings(full_df16)\n",
    "print(\"done with 16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12671b-163a-4a11-93fe-f5ec13991176",
   "metadata": {},
   "source": [
    "## Création du graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "116dda23-1691-4d98-b753-2f80d63e6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(df):\n",
    "    G = TextGraph(wv_threshold=.5, stopwords=[\"en\"]).fit(full_df12[\"Text\"], wv = wv)\n",
    "    A = nx.adjacency_matrix(G)\n",
    "    degrees = []\n",
    "    for d in G.degree(weight=None):\n",
    "        if d[1] == 0:\n",
    "            degrees.append(0)\n",
    "        else:\n",
    "            degrees.append(d[1]**(-0.5))\n",
    "\n",
    "    print(len(degrees))\n",
    "    print(A.shape)\n",
    "    degrees = np.diag(degrees)\n",
    "    A_hat = degrees @ A @ degrees\n",
    "    return G, A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab281e8-e73c-437d-a356-64eecc6c0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Modele avec co occurence\n",
    "\n",
    "#G= TextGraph(wv_threshold=.5, stopwords=[\"en\"]).fit(full_df12[\"Text\"])\n",
    "#nx.draw(G)\n",
    "#A = nx.adjacency_matrix(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013cfcf-ddb2-443a-9b89-d7ebe37ddb0f",
   "metadata": {},
   "source": [
    "## Création du GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a18fe717-d00c-4d74-b9b8-18c4495185bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class gcn(nn.Module):\n",
    "    def __init__(self, X_size, A_hat, num_classes, bias=True): # X_size = num features\n",
    "        super(gcn, self).__init__()\n",
    "        self.A_hat = torch.tensor(A_hat, requires_grad=False).float()\n",
    "        self.weight = nn.parameter.Parameter(torch.FloatTensor(X_size, hidden_size_1))\n",
    "        var = 2./(self.weight.size(1)+self.weight.size(0))\n",
    "        self.weight.data.normal_(0,var)\n",
    "        self.weight2 = nn.parameter.Parameter(torch.FloatTensor(hidden_size_1, hidden_size_2))\n",
    "        var2 = 2./(self.weight2.size(1)+self.weight2.size(0))\n",
    "        self.weight2.data.normal_(0,var2)\n",
    "        if bias:\n",
    "            self.bias = nn.parameter.Parameter(torch.FloatTensor(hidden_size_1))\n",
    "            self.bias.data.normal_(0,var)\n",
    "            self.bias2 = nn.parameter.Parameter(torch.FloatTensor(hidden_size_2))\n",
    "            self.bias2.data.normal_(0,var2)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.fc1 = nn.Linear(hidden_size_2, num_classes)\n",
    "        \n",
    "    def forward(self, X): ### 2-layer GCN architecture\n",
    "        X = torch.mm(X, self.weight)\n",
    "        if self.bias is not None:\n",
    "            X = (X + self.bias)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        X = torch.mm(X, self.weight2)\n",
    "        if self.bias2 is not None:\n",
    "            X = (X + self.bias2)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        return self.fc1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18fc2d83-538f-410a-bd43-75cffdfa0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_1 = 200\n",
    "hidden_size_2 = 100\n",
    "\n",
    "num_epochs=100\n",
    "lr=0.011\n",
    "model_no =0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fde962-448f-43b8-ac27-282fcc91b5b4",
   "metadata": {},
   "source": [
    "## Entrainement du GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d17af07-bf16-409a-bdf6-830ccfa012af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    completeName = os.path.join(\"save/\",\\\n",
    "                                filename)\n",
    "    with open(completeName, 'rb') as pkl_file:\n",
    "        data = pickle.load(pkl_file)\n",
    "    return data\n",
    "\n",
    "def save_as_pickle(filename, data):\n",
    "    completeName = os.path.join(\"save/\",\\\n",
    "                                filename)\n",
    "    with open(completeName, 'wb') as output:\n",
    "        pickle.dump(data, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03185db7-c9c0-4e21-99c5-6e0ec0c8d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(df):\n",
    "    map = {}\n",
    "    df2 = df.copy()\n",
    "    j = 0\n",
    "    for i in list(set(df[\"Manual_1\"])):\n",
    "        map[i] = j\n",
    "        j+=1\n",
    "    labelled = df[df[\"Manual_1\"] != \"\"]\n",
    "    df2.loc[:(len(labelled)-1),\"Manual_1\"] = [map[df[\"Manual_1\"][i]] for i in range(len(labelled))]\n",
    "    return map , df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9efbbd7f-1388-4689-9d46-933cf3cb712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "\n",
    "def evaluate(output, labels_e):\n",
    "    _, labels = output.max(1)\n",
    "    labels = labels.numpy()\n",
    "    return sum([(e) for e in labels_e] == labels)/len(labels)\n",
    "\n",
    "\n",
    "\n",
    "def train(data):\n",
    "    map, data = mapper(data)\n",
    "    print(map)\n",
    "\n",
    "    G, A_hat = generate_graph(data)\n",
    "\n",
    "    I = np.eye(A_hat.shape[0])\n",
    "    net = gcn(A_hat.shape[0], A_hat, num_classes = len(list(set(data[\"Manual_1\"]))) )\n",
    "\n",
    "    \n",
    "\n",
    "    f = torch.from_numpy(I).float()\n",
    "    selected = np.array(range(len(data[data[\"Manual_1\"] != \"\"]))) # Indices des datas labelisées\n",
    "    test_idx = np.random.choice(selected, size = 10)\n",
    "    \n",
    "    \n",
    "\n",
    "    selected = np.array(list(set(selected) - set(test_idx)))\n",
    "\n",
    "    \n",
    "    labels_selected = data.loc[selected, \"Manual_1\"]\n",
    "    labels_not_selected = data.loc[test_idx, \"Manual_1\"]\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000,2000,3000,4000,5000,6000], gamma=0.77)\n",
    "\n",
    "    start_epoch, best_pred = 0, 0\n",
    "    losses_per_epoch, evaluation_untrained = [], []\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    evaluation_trained = []\n",
    "    for e in range(start_epoch, num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(f)\n",
    "        loss = criterion(output[selected], torch.tensor(labels_selected).long())\n",
    "        losses_per_epoch.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e % 10 == 0:\n",
    "            ### Evaluate other untrained nodes and check accuracy of labelling\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_labels = net(f)\n",
    "                trained_accuracy = evaluate(output[selected], labels_selected)\n",
    "                untrained_accuracy = evaluate(pred_labels[test_idxs], labels_not_selected)\n",
    "            evaluation_trained.append((e, trained_accuracy))\n",
    "            evaluation_untrained.append((e, untrained_accuracy))\n",
    "            print(\"[Epoch \", e, \"]: Evaluation accuracy of trained nodes:\", trained_accuracy)\n",
    "            print(\"[Epoch \", e, \"]: Evaluation accuracy of test nodes:\", untrained_accuracy)\n",
    "            #print(\"Labels of trained nodes: \\n\", output[selected].max(1)[1])\n",
    "            net.train()\n",
    "            if trained_accuracy > best_pred:\n",
    "                best_pred = trained_accuracy\n",
    "                torch.save({\n",
    "                    'epoch': e + 1,\\\n",
    "                    'state_dict': net.state_dict(),\\\n",
    "                    'best_acc': trained_accuracy,\\\n",
    "                    'optimizer' : optimizer.state_dict(),\\\n",
    "                    'scheduler' : scheduler.state_dict(),\\\n",
    "                }, os.path.join(\"save/\" ,\\\n",
    "                \"test_model_best_%d.pth.tar\" % model_no))\n",
    "    scheduler.step()\n",
    "    net(f)\n",
    "    return output.max(1)[1], map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c3457dc-5998-4cca-af23-67fcf78f8a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0, '15.c': 1, '': 2, '0.0': 3, 15.6: 4, 15.3: 5, 15.5: 6, 15.1: 7, 15.2: 8, '15.2': 9, '15.6': 10, '15.7': 11, '15.8': 12, '15.5': 13, '15.a': 14, '15.b': 15, '15.1': 16, '15.4': 17, '15.3': 18, '15.9': 19}\n",
      "5583\n",
      "(5583, 5583)\n",
      "[Epoch  0 ]: Evaluation accuracy of trained nodes: 0.047619047619047616\n",
      "[Epoch  0 ]: Evaluation accuracy of test nodes: 0\n",
      "[Epoch  10 ]: Evaluation accuracy of trained nodes: 0.23015873015873015\n",
      "[Epoch  10 ]: Evaluation accuracy of test nodes: 0.03895377781711676\n",
      "[Epoch  20 ]: Evaluation accuracy of trained nodes: 0.3253968253968254\n",
      "[Epoch  20 ]: Evaluation accuracy of test nodes: 0.16504250861183756\n",
      "[Epoch  30 ]: Evaluation accuracy of trained nodes: 0.49206349206349204\n",
      "[Epoch  30 ]: Evaluation accuracy of test nodes: 0.3216584816047163\n",
      "[Epoch  40 ]: Evaluation accuracy of trained nodes: 0.5714285714285714\n",
      "[Epoch  40 ]: Evaluation accuracy of test nodes: 0.4055918912607606\n",
      "[Epoch  50 ]: Evaluation accuracy of trained nodes: 0.6031746031746031\n",
      "[Epoch  50 ]: Evaluation accuracy of test nodes: 0.4525036676778882\n",
      "[Epoch  60 ]: Evaluation accuracy of trained nodes: 0.6031746031746031\n",
      "[Epoch  60 ]: Evaluation accuracy of test nodes: 0.4503853766859745\n",
      "[Epoch  70 ]: Evaluation accuracy of trained nodes: 0.6031746031746031\n",
      "[Epoch  70 ]: Evaluation accuracy of test nodes: 0.4256186003469913\n",
      "[Epoch  80 ]: Evaluation accuracy of trained nodes: 0.6031746031746031\n",
      "[Epoch  80 ]: Evaluation accuracy of test nodes: 0.40922781964646326\n",
      "[Epoch  90 ]: Evaluation accuracy of trained nodes: 0.6031746031746031\n",
      "[Epoch  90 ]: Evaluation accuracy of test nodes: 0.44020106874453113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  7,  ...,  1, 16, 16]),\n",
       " {0.0: 0,\n",
       "  '15.c': 1,\n",
       "  '': 2,\n",
       "  '0.0': 3,\n",
       "  15.6: 4,\n",
       "  15.3: 5,\n",
       "  15.5: 6,\n",
       "  15.1: 7,\n",
       "  15.2: 8,\n",
       "  '15.2': 9,\n",
       "  '15.6': 10,\n",
       "  '15.7': 11,\n",
       "  '15.8': 12,\n",
       "  '15.5': 13,\n",
       "  '15.a': 14,\n",
       "  '15.b': 15,\n",
       "  '15.1': 16,\n",
       "  '15.4': 17,\n",
       "  '15.3': 18,\n",
       "  '15.9': 19})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(full_df15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb253ac6-364b-47ad-a73b-8a0e590be6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'16.10': 0, 0: 1, '': 2, '16.4': 3, '16.a': 4, '16.8': 5, '16.2': 6, '0': 7, '16.6': 8, '16.b': 9, '16.5': 10, '16.1': 11, '16.9': 12, '16.7': 13, '16.3': 14}\n",
      "5583\n",
      "(5583, 5583)\n",
      "[Epoch  0 ]: Evaluation accuracy of trained nodes: 0.04411764705882353\n",
      "[Epoch  0 ]: Evaluation accuracy of test nodes: 0\n",
      "[Epoch  10 ]: Evaluation accuracy of trained nodes: 0.4485294117647059\n",
      "[Epoch  10 ]: Evaluation accuracy of test nodes: 0.27531079160977207\n",
      "[Epoch  20 ]: Evaluation accuracy of trained nodes: 0.7132352941176471\n",
      "[Epoch  20 ]: Evaluation accuracy of test nodes: 0.54241338736425\n",
      "[Epoch  30 ]: Evaluation accuracy of trained nodes: 0.8382352941176471\n",
      "[Epoch  30 ]: Evaluation accuracy of test nodes: 0.6485992464387235\n",
      "[Epoch  40 ]: Evaluation accuracy of trained nodes: 0.9191176470588235\n",
      "[Epoch  40 ]: Evaluation accuracy of test nodes: 0.7339660626462265\n",
      "[Epoch  50 ]: Evaluation accuracy of trained nodes: 0.9191176470588235\n",
      "[Epoch  50 ]: Evaluation accuracy of test nodes: 0.7203979717140057\n",
      "[Epoch  60 ]: Evaluation accuracy of trained nodes: 0.9191176470588235\n",
      "[Epoch  60 ]: Evaluation accuracy of test nodes: 0.7193005227518909\n",
      "[Epoch  70 ]: Evaluation accuracy of trained nodes: 0.9191176470588235\n",
      "[Epoch  70 ]: Evaluation accuracy of test nodes: 0.7351920444251407\n",
      "[Epoch  80 ]: Evaluation accuracy of trained nodes: 0.9191176470588235\n",
      "[Epoch  80 ]: Evaluation accuracy of test nodes: 0.73014276699147\n",
      "[Epoch  90 ]: Evaluation accuracy of trained nodes: 0.9191176470588235\n",
      "[Epoch  90 ]: Evaluation accuracy of test nodes: 0.7265278402727278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  8, 10,  ...,  0,  9,  9]),\n",
       " {'16.10': 0,\n",
       "  0: 1,\n",
       "  '': 2,\n",
       "  '16.4': 3,\n",
       "  '16.a': 4,\n",
       "  '16.8': 5,\n",
       "  '16.2': 6,\n",
       "  '0': 7,\n",
       "  '16.6': 8,\n",
       "  '16.b': 9,\n",
       "  '16.5': 10,\n",
       "  '16.1': 11,\n",
       "  '16.9': 12,\n",
       "  '16.7': 13,\n",
       "  '16.3': 14})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(full_df16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0727faf-d7d8-4b57-b50c-b5b947a8696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, '': 1, '12.a': 2, '12.1': 3, '12.2': 4, '12.3': 5, '12.8': 6, '12.b': 7, '12.c': 8, '12.5': 9, '12.6': 10, '12.7': 11, '12.4': 12, '0': 13}\n",
      "5583\n",
      "(5583, 5583)\n",
      "[Epoch  0 ]: Evaluation accuracy of trained nodes: 0.04032258064516129\n",
      "[Epoch  0 ]: Evaluation accuracy of test nodes: 0\n",
      "[Epoch  10 ]: Evaluation accuracy of trained nodes: 0.49193548387096775\n",
      "[Epoch  10 ]: Evaluation accuracy of test nodes: 0.31032624747786325\n",
      "[Epoch  20 ]: Evaluation accuracy of trained nodes: 0.6774193548387096\n",
      "[Epoch  20 ]: Evaluation accuracy of test nodes: 0.5249016054858532\n",
      "[Epoch  30 ]: Evaluation accuracy of trained nodes: 0.8548387096774194\n",
      "[Epoch  30 ]: Evaluation accuracy of test nodes: 0.6727774250003\n",
      "[Epoch  40 ]: Evaluation accuracy of trained nodes: 0.8790322580645161\n",
      "[Epoch  40 ]: Evaluation accuracy of test nodes: 0.7242068828498807\n",
      "[Epoch  50 ]: Evaluation accuracy of trained nodes: 0.9435483870967742\n",
      "[Epoch  50 ]: Evaluation accuracy of test nodes: 0.7530391033255852\n",
      "[Epoch  60 ]: Evaluation accuracy of trained nodes: 0.9516129032258065\n",
      "[Epoch  60 ]: Evaluation accuracy of test nodes: 0.7963241463546649\n",
      "[Epoch  70 ]: Evaluation accuracy of trained nodes: 0.9516129032258065\n",
      "[Epoch  70 ]: Evaluation accuracy of test nodes: 0.7633598513568214\n",
      "[Epoch  80 ]: Evaluation accuracy of trained nodes: 0.9516129032258065\n",
      "[Epoch  80 ]: Evaluation accuracy of test nodes: 0.7993627823434261\n",
      "[Epoch  90 ]: Evaluation accuracy of trained nodes: 0.9516129032258065\n",
      "[Epoch  90 ]: Evaluation accuracy of test nodes: 0.7935856306617239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([12, 12,  9,  ...,  7,  7,  3]),\n",
       " {0: 0,\n",
       "  '': 1,\n",
       "  '12.a': 2,\n",
       "  '12.1': 3,\n",
       "  '12.2': 4,\n",
       "  '12.3': 5,\n",
       "  '12.8': 6,\n",
       "  '12.b': 7,\n",
       "  '12.c': 8,\n",
       "  '12.5': 9,\n",
       "  '12.6': 10,\n",
       "  '12.7': 11,\n",
       "  '12.4': 12,\n",
       "  '0': 13})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(full_df12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2930162-1747-47a6-97cf-aefa93f83f73",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a53e13-b984-4ca9-88bd-ccc28438fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\"data/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24c68fd0-ed0c-4e7a-9d8f-571c5d193b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_csv(\"data/test/processed_odd15.csv\", sep = \";\").drop(columns = \"Unnamed: 0\")\n",
    "empty_cols = [\"\" for i in range(len(df_target))]\n",
    "\n",
    "df_target[\"Manual_1\"] =empty_cols\n",
    "df_target[\"Manual_2 \"] = empty_cols\n",
    "df_target.columns = [\"Text\", \"Manual_1\", \"Manual_2 \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b0f3c25-3aba-4e47-a94a-c53299492209",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df15 = pd.concat([full_df15, df_target]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a70e489-71f0-4af0-9b4f-cecd59aabdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = generate_embeddings(test_df15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "241e2ec4-0f52-47b3-8864-17f878727ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0, '': 1, '15.a': 2, '15.c': 3, '15.8': 4, 15.6: 5, 15.3: 6, 15.5: 7, 15.1: 8, 15.2: 9, '15.6': 10, '15.1': 11, '15.b': 12, '15.2': 13, '15.9': 14, '15.7': 15, '15.4': 16, '15.5': 17, '0.0': 18, '15.3': 19}\n",
      "5583\n",
      "(5583, 5583)\n",
      "range(0, 126)\n",
      "[Epoch 0]: Evaluation accuracy of trained nodes: 0.1111111\n",
      "Labels of trained nodes: \n",
      " tensor(2.9993, grad_fn=<NllLossBackward0>) tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18])\n",
      "[Epoch 10]: Evaluation accuracy of trained nodes: 0.2539683\n",
      "Labels of trained nodes: \n",
      " tensor(2.2258, grad_fn=<NllLossBackward0>) tensor([18, 18, 11,  8, 11,  8, 11, 11, 18, 11,  8, 11, 18, 18, 11, 18,  8, 11,\n",
      "        18,  8, 11, 18,  8, 11, 11,  8, 18, 11, 11, 11, 11, 18,  8,  8, 11,  8,\n",
      "        18, 18, 18, 11, 18, 18, 11, 18,  8, 11, 18, 11,  8, 18, 18, 18, 11,  8,\n",
      "        11,  8, 11, 11, 18, 11,  8, 11, 18, 18, 11, 18,  8, 11, 18,  8, 11, 18,\n",
      "         8, 11, 11,  8, 18, 11, 11, 11, 11, 18,  8,  8, 11,  8, 18, 18, 18, 11,\n",
      "        18, 18, 11, 18,  8, 11, 18, 11,  8, 18, 11, 11, 11, 18, 18,  8,  8, 11,\n",
      "         8, 18, 11, 11,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8])\n",
      "[Epoch 20]: Evaluation accuracy of trained nodes: 0.3650794\n",
      "Labels of trained nodes: \n",
      " tensor(1.5709, grad_fn=<NllLossBackward0>) tensor([ 0, 18,  8, 17,  7, 17,  7, 13,  0, 11, 11, 11, 13,  0, 11,  0, 13, 11,\n",
      "         0, 13, 11,  0,  8, 13, 17, 11,  0, 11,  7, 11, 11,  7, 13,  8, 11, 11,\n",
      "         0,  0, 18, 11, 18,  0,  7, 11, 13, 13,  0,  7, 13,  0,  0, 18,  8, 17,\n",
      "         7, 17,  7, 13,  0, 11, 11, 11, 13,  0, 11,  0, 13, 11,  0, 13, 11,  0,\n",
      "         8, 13, 17, 11,  0, 11,  7, 11, 11,  7, 13,  8, 11, 11,  0,  0, 18, 11,\n",
      "        18,  0,  7, 11, 13, 13,  0,  7, 13,  0, 11, 11, 11, 13, 13, 13, 13, 13,\n",
      "        13, 13, 11, 11, 13,  8, 13, 17, 13, 13, 17, 13, 17, 13, 13, 13, 17, 17])\n",
      "[Epoch 30]: Evaluation accuracy of trained nodes: 0.4920635\n",
      "Labels of trained nodes: \n",
      " tensor(1.0394, grad_fn=<NllLossBackward0>) tensor([ 0,  0,  8, 19, 17, 10, 17,  9,  0,  8, 11,  8, 10,  0,  8,  0, 13, 11,\n",
      "         0, 10,  8,  0,  8, 13, 19,  8,  0,  8, 17,  8,  8, 17, 13,  8, 17,  8,\n",
      "         0,  0,  5,  8,  0,  0, 17, 19, 13, 19,  0,  7, 13,  0,  0,  0,  8, 19,\n",
      "        17, 10, 17,  9,  0,  8, 11,  8, 10,  0,  8,  0, 13, 11,  0, 10,  8,  0,\n",
      "         8, 13, 19,  8,  0,  8, 17,  8,  8, 17, 13,  8, 17,  8,  0,  0,  5,  8,\n",
      "         0,  0, 17, 19, 13, 19,  0,  7, 13,  0, 11, 11, 11, 13, 13, 19, 19,  5,\n",
      "        10, 16, 17, 17, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "[Epoch 40]: Evaluation accuracy of trained nodes: 0.5634921\n",
      "Labels of trained nodes: \n",
      " tensor(0.7277, grad_fn=<NllLossBackward0>) tensor([ 0,  0,  8, 19,  7, 10, 17, 13,  0,  8, 11,  8, 10,  0,  8,  0, 13, 11,\n",
      "         0, 10, 11,  0,  8, 13,  6, 11,  0,  8, 17, 11, 11, 17, 13,  8, 17,  8,\n",
      "         0,  0, 10,  8,  0,  0, 17, 19, 13,  6,  0,  7, 13,  0,  0,  0,  8, 19,\n",
      "         7, 10, 17, 13,  0,  8, 11,  8, 10,  0,  8,  0, 13, 11,  0, 10, 11,  0,\n",
      "         8, 13,  6, 11,  0,  8, 17, 11, 11, 17, 13,  8, 17,  8,  0,  0, 10,  8,\n",
      "         0,  0, 17, 19, 13,  6,  0,  7, 13,  0, 11, 11, 11, 13, 13, 19, 19, 16,\n",
      "        16, 16, 17, 17, 10, 10, 15, 15, 10, 10,  3, 14, 15, 15, 12, 12,  3,  3])\n",
      "[Epoch 50]: Evaluation accuracy of trained nodes: 0.6031746\n",
      "Labels of trained nodes: \n",
      " tensor(0.6275, grad_fn=<NllLossBackward0>) tensor([ 0,  0,  8, 19, 17, 10, 17, 13,  0,  8,  8,  8, 10,  0,  8,  0, 13, 11,\n",
      "         0, 10,  8,  0,  8, 13, 19,  8,  0,  8, 17,  8,  8, 17, 13,  8, 17,  8,\n",
      "         0,  0, 10,  8,  0,  0,  7, 19, 13, 19,  0, 17, 13,  0,  0,  0,  8, 19,\n",
      "        17, 10, 17, 13,  0,  8,  8,  8, 10,  0,  8,  0, 13, 11,  0, 10,  8,  0,\n",
      "         8, 13, 19,  8,  0,  8, 17,  8,  8, 17, 13,  8, 17,  8,  0,  0, 10,  8,\n",
      "         0,  0,  7, 19, 13, 19,  0, 17, 13,  0, 11, 11, 11, 13, 13, 19, 19, 16,\n",
      "        16, 16, 17, 17, 10, 10, 15, 15,  4,  4, 14, 14,  2,  2, 12, 12,  3,  3])\n",
      "[Epoch 60]: Evaluation accuracy of trained nodes: 0.6031746\n",
      "Labels of trained nodes: \n",
      " tensor(0.5660, grad_fn=<NllLossBackward0>) tensor([ 0,  0,  8, 19, 17,  5, 17, 13,  0,  8,  8,  8,  5,  0,  8,  0, 13, 11,\n",
      "         0,  5,  8,  0,  8, 13, 19,  8,  0,  8, 17, 11, 11, 17, 13,  8, 17,  8,\n",
      "         0,  0,  5,  8,  0,  0, 17, 19, 13, 19,  0, 17, 13,  0,  0,  0,  8, 19,\n",
      "        17,  5, 17, 13,  0,  8,  8,  8,  5,  0,  8,  0, 13, 11,  0,  5,  8,  0,\n",
      "         8, 13, 19,  8,  0,  8, 17, 11, 11, 17, 13,  8, 17,  8,  0,  0,  5,  8,\n",
      "         0,  0, 17, 19, 13, 19,  0, 17, 13,  0, 11, 11, 11, 13, 13, 19, 19, 16,\n",
      "        16, 16, 17, 17, 10, 10, 15, 15,  4,  4, 14, 14,  2,  2, 12, 12,  3,  3])\n",
      "[Epoch 70]: Evaluation accuracy of trained nodes: 0.6031746\n",
      "Labels of trained nodes: \n",
      " tensor(0.5579, grad_fn=<NllLossBackward0>) tensor([ 0,  0, 11, 19, 17,  5, 17, 13,  0, 11, 11, 11,  5,  0, 11,  0, 13,  8,\n",
      "         0,  5, 11,  0, 11, 13, 19, 11,  0, 11, 17, 11, 11, 17, 13, 11, 17, 11,\n",
      "         0,  0,  5, 11,  0,  0, 17, 19, 13, 19,  0, 17, 13,  0,  0,  0, 11, 19,\n",
      "        17,  5, 17, 13,  0, 11, 11, 11,  5,  0, 11,  0, 13,  8,  0,  5, 11,  0,\n",
      "        11, 13, 19, 11,  0, 11, 17, 11, 11, 17, 13, 11, 17, 11,  0,  0,  5, 11,\n",
      "         0,  0, 17, 19, 13, 19,  0, 17, 13,  0, 11, 11, 11, 13, 13, 19, 19, 16,\n",
      "        16, 16, 17, 17, 10, 10, 15, 15,  4,  4, 14, 14,  2,  2, 12, 12,  3,  3])\n",
      "[Epoch 80]: Evaluation accuracy of trained nodes: 0.6031746\n",
      "Labels of trained nodes: \n",
      " tensor(0.5534, grad_fn=<NllLossBackward0>) tensor([ 0,  0, 11,  6, 17, 10,  7, 13,  0, 11, 11, 11, 10,  0, 11,  0, 13, 11,\n",
      "         0, 10, 11,  0, 11, 13,  6, 11,  0, 11, 17,  8, 11,  7, 13, 11,  7, 11,\n",
      "         0,  0, 10, 11,  0,  0, 17,  6, 13,  6,  0, 17, 13,  0,  0,  0, 11,  6,\n",
      "        17, 10,  7, 13,  0, 11, 11, 11, 10,  0, 11,  0, 13, 11,  0, 10, 11,  0,\n",
      "        11, 13,  6, 11,  0, 11, 17,  8, 11,  7, 13, 11,  7, 11,  0,  0, 10, 11,\n",
      "         0,  0, 17,  6, 13,  6,  0, 17, 13,  0, 11, 11, 11, 13, 13, 19, 19, 16,\n",
      "        16, 16, 17, 17, 10, 10, 15, 15,  4,  4, 14, 14,  2,  2, 12, 12,  3,  3])\n",
      "[Epoch 90]: Evaluation accuracy of trained nodes: 0.6031746\n",
      "Labels of trained nodes: \n",
      " tensor(0.5524, grad_fn=<NllLossBackward0>) tensor([ 0,  0, 11, 19, 17,  5, 17, 13,  0, 11, 11, 11,  5,  0, 11,  0, 13, 11,\n",
      "         0,  5, 11,  0, 11, 13, 19, 11,  0, 11, 17, 11, 11, 17, 13, 11, 17, 11,\n",
      "         0,  0,  5, 11,  0,  0, 17, 19, 13, 19,  0, 17, 13,  0,  0,  0, 11, 19,\n",
      "        17,  5, 17, 13,  0, 11, 11, 11,  5,  0, 11,  0, 13, 11,  0,  5, 11,  0,\n",
      "        11, 13, 19, 11,  0, 11, 17, 11, 11, 17, 13, 11, 17, 11,  0,  0,  5, 11,\n",
      "         0,  0, 17, 19, 13, 19,  0, 17, 13,  0, 11, 11, 11, 13, 13, 19, 19, 16,\n",
      "        16, 16, 17, 17, 10, 10, 15, 15,  4,  4, 14, 14,  2,  2, 12, 12,  3,  3])\n"
     ]
    }
   ],
   "source": [
    "out, map = train(test_df15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd6d2f79-7f3f-449e-8aa3-274d4c673b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17, 17,  7,  ..., 14,  9,  9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0500e59-be4a-46c2-9532-7f6dd725eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n",
      "tensor(9)\n",
      "tensor(16)\n",
      "tensor(16)\n",
      "tensor(16)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_target)):\n",
    "    print(out[-i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f349df2-1cb0-4661-ab14-14413f600eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0,\n",
       " '15.5': 1,\n",
       " '': 2,\n",
       " '15.8': 3,\n",
       " 15.6: 4,\n",
       " 15.3: 5,\n",
       " 15.5: 6,\n",
       " 15.1: 7,\n",
       " 15.2: 8,\n",
       " '15.1': 9,\n",
       " '15.3': 10,\n",
       " '15.6': 11,\n",
       " '15.b': 12,\n",
       " '15.7': 13,\n",
       " '15.c': 14,\n",
       " '15.9': 15,\n",
       " '15.a': 16,\n",
       " '0.0': 17,\n",
       " '15.4': 18,\n",
       " '15.2': 19}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09b615-3eae-454c-8b10-74b39e24a0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
